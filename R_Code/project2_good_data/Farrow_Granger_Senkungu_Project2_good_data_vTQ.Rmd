---
title: "Stats 2 Project 2"
author: "Rick Farrow, Bruce Granger, TQ Senkungu"
date: "12/1/2018"
output:
    html_document:
        keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Required Libraries
- Tidyverse 
- Amelia
- Corrplot
- PSCL

```{r Load_Libraries, include=FALSE}
# function to install any missing packages if needd to run knitr

install.if.nec <- function(x){
  for( i in x ){
    #  require returns TRUE invisibly if it was able to load package
    if( ! require( i , character.only = TRUE ) ){
      #  If package was not able to be loaded then re-install
      install.packages( i , dependencies = TRUE, repos = "http://cran.us.r-project.org")
      #  Load package after installing
      require( i , character.only = TRUE )
    }
  }
}

#  Try/install necessary packages
# Use this function wherever you make use of other packages.
install.if.nec( c("tidyverse", "Amelia", "corrplot", "pscl", "ROCR", "MASS", "car", "ggplot2", "rgl", "tree", "ISLR", "randomForest", "class", "Metrics") )
#  Try/install necessary packages

library(ROCR)
library(MASS)
library(tidyverse)
library(Amelia)
library(corrplot)
library(pscl)
library(car)
library(ggplot2)
library(rgl)
library(tree)
library(ISLR)
library(randomForest) 
library(class)
library(Metrics)
```

## Load Data

- We are using the Breast Cancer Data from the Wisconsin Diagnostic Breast Cancer (WDBC) dataset and will first load it.

```{r input_data}
wdbc_data <- read.csv("https://raw.githubusercontent.com/tikisen/6372_proj2/master/Data/wdbc.data1.csv", 
                      sep = ",", 
                      row.names = NULL, 
                      header = TRUE,
                      na.strings = c(""),
                      stringsAsFactors = FALSE)
wdbc_data <- wdbc_data %>% dplyr::select(ID_number = 1, everything())
wdbc_sub <- wdbc_data %>% dplyr::select(1:12)
wdbc_sub <- wdbc_sub %>% mutate(dia = case_when(diagnosis == "B" ~ 0,
                                                  diagnosis == "M" ~ 1)) %>%
  select(1, 3:13) %>% select(1, diagnosis = dia, 2:11)

```

# Bruce's Work Starts Here

## Check for missing values
```{r CheckForMissingValues}
sapply(wdbc_data,function(x) sum(is.na(x)))
```

- The above output identifies there is missing data in each of the attributes, but what is not clear at this point if the missing values are for the same of different IDs.

- The following visualizes where the missing data is occurring:

## Summary Statistics/Histograms
```{r SummaryStats}
# TOTAL COUNT BY CANCER TYPE ####
ggplot(data=wdbc_sub, aes(x=diagnosis, colour = diagnosis)) +
  geom_bar() +
  geom_text(stat='Count', aes(label=..count..), vjust = 5) +
  theme(legend.position = "none") +
  scale_color_manual(values = c("blue", "red")) +
  ggtitle("Total Count by Cancer Type: Blue = Benign; Red = Malignant ") 
  

# PERCENT OF TOTAL BY CANCER TYPE ####
wdbc_data.percent <- wdbc_sub %>% 
  count(diagnosis) %>% 
  mutate(perc = n / nrow(wdbc_sub))

ggplot(data=wdbc_data.percent, aes(x = diagnosis, y = perc, colour = diagnosis)) +
  geom_bar(stat = "identity") +
  geom_text(stat = "identity", aes(label=round(perc*100,2)), vjust = 10) +
  theme(legend.position = "none") +
  scale_color_manual(values = c("blue", "red")) +
  ggtitle("Percent of Total by Cancer Type: Blue = Benign; Red = Malignant ")
rm(wdbc_data.percent) 
```

- Ideally, the proportion of events and non-events in the Y variable should approximately be the same.
- From the above histograms it is obvious there is a class bias, a condition observed when the Malignant proportion of events is much smaller than proportion of Benign events, by a factor of 1 to 1.9.  
- As a result, we must sample the observations in approximately equal proportions in order to get better model.  Unfortunately we have a small sample size, which means we are not able to implement this strategy. 
 
## Pairs Plots
- Version 1:
- The objective of the "Pairs Plot" is to create plots based upon paring of variables
- Additionaly, each observation is color coded to simutaleniously see if the observation is "Benign" or "Malignant " cancer.  
  - Color Coding:
    * Blue = Benign 
    * Red = Malignant 

```{r pairsplot_ver1}
ed.small <- wdbc_sub %>% dplyr::select(-c(1))

cols <- character(nrow(ed.small))
cols[] <- "black"
cols[ed.small$diagnosis == 0] <- "blue"
cols[ed.small$diagnosis == 1] <- "red"
pairs(ed.small, col=cols, main = "WDBC Pairs Plot: Blue = Benign; Red = Malignant ")
rm(cols)
```

- Version 2:
- Similar to the first Pairs Plot, however version 2 introduces jitter to the observations and as a result it is easy to see the density of the observations.

```{r pairsplot_ver2}

# the alpha argument in rgb() lets you set the transparency
cols2 = c(rgb(red=0, green=0, blue=255, alpha=50, maxColorValue=255), 
          rgb(red=255, green=0, blue=0, alpha=50, maxColorValue=255))
cols2 = ifelse(ed.small$diagnosis==0, cols2[1], cols2[2])

# here we jitter the data
set.seed(6141)  # this makes the example exactly reproducible
jbreast = apply(ed.small[,2:10], 2, FUN=function(x){ jitter(x, amount=.5) })
jbreast = cbind(jbreast, class=ed.small[,11])  # the class variable is not jittered

#windows()  # to match up the 1st & 2nd sets requires more coding
layout(matrix(1:25, nrow=5, byrow=T))
par(mar=c(.5,.5,.5,.5), oma=c(2,2,2,2))

for(i in 1:5){
   for(j in 6:10){
    
    plot(jbreast[,j], jbreast[,i], col=cols2, pch=16,
         
         axes=F, main="", xlab="", ylab="")
    
    box()
    
    if(j==6 ){ mtext(colnames(jbreast)[i], side=2, cex=.7, line=1) }
    
    if(i==5 ){ mtext(colnames(jbreast)[j], side=1, cex=.7, line=1) }
    
    if(j==10){ axis(side=4, seq(2,10,2), cex.axis=.8) }
    
    if(i==1 ){ axis(side=3, seq(2,10,2), cex.axis=.8) }
    
  }
  
}
rm(list = c("jbreast", "cols2", "i", "j"))

```

## Corrleation Plot
- Positive correlations are displayed in blue and negative correlations in red color.
- Color intensity and the size of the circle are proportional to the correlation coefficients.

```{r CorrelationPlot}
corr <- cor(ed.small)
corrplot(corr, method ="number", type= "upper")
corrplot(corr, method ="circle", type= "upper")
rm(corr)
```

## Histogram
- The histograms are a look at each attribute broken down by diagnosis

```{r histogram}
ed.small <- entire.dataset %>% dplyr::select(-c(1,11))

ed.tall <- ed.small %>%
  gather(-10, key = "Variable", value = "Value") %>% 
  filter(!is.na(diagnosis))

ggplot(data = ed.tall, aes(x=Value)) +
  geom_histogram(bins=15) + 
  facet_wrap(Variable ~ diagnosis, ncol = 9)
```

## Boxplots
- Boxplot of attributes, color by diagnosis

```{r Boxplot}
ed.tall <- ed.small %>% 
  gather(-10, key = "Variable", value = "Value") %>% 
  filter(!is.na(diagnosis))

ggplot(ed.tall, aes(x=diagnosis, y=Value, fill = diagnosis)) + 
  geom_boxplot() +
  facet_wrap(~ Variable) +
  ggtitle("WDBC Boxplot ") + 
  scale_fill_manual(breaks = c("Benign", "Malignant "), values = c("blue", "red")) +
  theme(legend.position="none")
```

## MODEL SELECTION
- In stepwise regression, the full model is passed to a *step* function. The stepwise function iteratively searches the full scope of variables.
- The iteration will be performed in a forward and backwards directions.
- In a backward selection, the *step* function performs multiple iteractions by droping one independent variable at a time.  
- In a forward selection, the *step* function performs multiple iteractions by adding one independent variable at a time.
- In each (forward and backward) iteration, multiple models are built and the AIC of the models is computed and the model that yields the lowest AIC is retained for the next iteration.  

```{r StepwiseForwardSelection}
wdbc.data.2 <- wdbc.data %>% dplyr::select(-ID)

lmMod <- lm(Class ~ . , data = wdbc.data.2)
selectedMod <- step(lmMod, direction = "forward")

all.vifs <- car::vif(selectedMod)
print(all.vifs)
```

- When using a forward selection, all nine variables are retained in the model selection.

```{r StepwiseBackwardSelection}
selectedMod <- step(lmMod, direction = "backward")
all.vifs <- car::vif(selectedMod)
print(all.vifs)
```

- When using a backward selection, eight variables are retained in the model selection, only Mitoses is omitted from the model selection.

## Logistic Regression
- Following the established discipline of dividing a portion of the dataset into a training set and the remainder into a test set.
- The training set will be used to build the model and the test set will be used to validate the model.
- The training set will consist of 20% of the data and the test set will consist of the remaining 80%.
- A set.seed is used at the beginning of the test and training division in order to get reproducible results.
```{r LogisticRegression}
wdbc.data.2 <-  wdbc.data %>% mutate(Class2 = case_when(Class == 2 ~ 0,
                                                        Class == 4 ~ 1)) %>%
 dplyr::select(texture, Cell_Size, Cell_Shape, Adhesion, Epithelial, Nuclei,
               Chromatin, Nucleoli, Mitoses, Class = Class2)

wdbc.data.2$Class <- as.factor(wdbc.data.2$Class)

set.seed(12345) #to get repeatable data

#wdbc.train <- sample_frac(ed.small, 0.2, replace = FALSE)
wdbc.train <- sample_frac(wdbc.data.2, 0.2, replace = FALSE)

train.index <- as.numeric(rownames(wdbc.train))
#wdbc.test <- ed.small[-train.index,]
wdbc.test <- wdbc.data.2[-train.index,]

rm(train.index)

# MODEL FITTING
# wdbc.glm.fit <- glm(diagnosis ~ ., data = wdbc.train, family = binomial(link = "logit"))
# wdbc.glm.fit <- glm(diagnosis ~ ., data = wdbc.train, family = "binomial")
wdbc.glm.fit <- glm(Class ~ ., data = wdbc.train, family = "binomial")
summary(wdbc.glm.fit)
```

Assessing Model Fit:

The following attributes are NOT statistically significant:  
* Cell_Size  
* Epithelial  
  
The following attributes ARE statistically significant:  
* Clump  
* Nuclei  
* Chromatin  
* Adhesion  
* Cell_Shape  
* Mitoses  
  
- In this logit model, the response valirable (diagnosis) is log odds, a unit increase in Clump increases the odds by 0.6, a unit increase in Nuclei increases the odds by 0.4, and a unit increase in Chromatin increases the odds by 0.5.
- The null deviance (the deviance just for the mean) is 1271 and the residual deviance (the deviance for the model) is 171.
- The difference between the null deviance and the residual deviance shows how the model is doing against the null model (a model with only the intercept). The wider the gap between the null deviance and the residual deviance, the better.
- Analyzing the table we can see the drop in deviance when adding each variable one at a time, with the exception of when adding Nuclei and Chromatin.
- With the addition of Clump significantly reduces the residual deviance and each additional attribute reduces the residual deviance, but in much smaller increments.

```{r GLM_FIT_ANOVA}
anova(wdbc.glm.fit, test = "Chisq")
```

Assessing Table of Deviance (ToD):

From the ToD we are able to see the difference between the null deviance versus the residual deviance.  The ToD shows how our model is doing against the null model, which is a model with only the intercept. The wider this gap between the model versus the null, the better.

By adding Clump and Cell_Size to the model drastically reduces the residual deviance.  All attributes have 0.05 p-value or less.

Eventhough there is not an exact equivalent to the R2 of linear regression exists, the McFadden R2 index can be used to assess the model fit:
```{r ModelFit}
pr2_mc <- pR2(wdbc.glm.fit)
pr2_mc[[4]]
```

Assessing the predictive ability of the logistic regression model by predicting from test dataset using against the training dataset:
- The predict function provides probabilities of classification
- Using a probabilities from the predict function, the ifelse will use the threshold of 0.5 to assign classification of "Malignant ", else it will assign a classification of "Benign"
- A confusion matrix will provide how well the model is doing
```{r PredictTest}
glm.probs <- predict(wdbc.glm.fit, wdbc.test, type = "response")

prediction(glm.probs, wdbc.test$Class) -> pred_log

performance(pred_log, "acc") -> acc
plot(acc) 
plot(glm.probs)

# MAX Accuracy ~ 0.1
# Confusion Matrix Using Max Accuracy
x <- table(wdbc.test$Class,glm.probs > 0.6)
x

# Accuracy when using Max Accuracy Cutoff is:
(x[1] + x[4])/(x[1] + x[2] + x[3] + x[4])

# TP and FP Rates When Using Max Accuracy Cutoff when cutoff:
# TP Rate:
x[4]/(x[4] + x[2])
# FP Rate:
x[3]/(x[3] + x[1])

# Build ROC to get good tradoff between Accuracy and TP/FP Rate
performance(pred_log, "tpr", "fpr") -> roc_curve
plot(roc_curve, colorize=T)

glm.pred <- ifelse(glm.probs > 0.39, "Malignant ","Benign")

# We are attempting to predict diagnosis variabel in the dataset, this is used to evaluate what is predicted in the model to the actual state
attach(wdbc.test)
table(glm.pred,Class)

```
- The confusion matrix operates on the diagonial where the model predicted **CORRECTLY**, starting at the top left (726) and the bottom right (335).
- The off diagonial is where the model predicted **INCORRECTLY**, starting at the bottom left (39) and the top right (18).
- Out of 1118 records, the model had 1070 accurate predictions for an accuracy rate of 95%

# Bruce's Work Ends Here

# Rick's Work Starts Here

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}

```

# Rick's Work Ends Here

#TQ's Work Starts Here


##Proportions

```{r proportions}
attach(wdbc_sub)
#table of counts
ftable(addmargins(table(diagnosis, texture)))
ftable(addmargins(table(diagnosis, radius)))
ftable(addmargins(table(diagnosis, perimeter)))
ftable(addmargins(table(diagnosis, area)))
ftable(addmargins(table(diagnosis, smoothness)))
ftable(addmargins(table(diagnosis, compactness)))
ftable(addmargins(table(diagnosis, concavity)))
ftable(addmargins(table(diagnosis, concave_points)))
ftable(addmargins(table(diagnosis, symmetry)))
ftable(addmargins(table(diagnosis, fractal_dimension)))


#in proprtions
prop.table(table(diagnosis, texture),2)
prop.table(table(diagnosis, radius),2)
prop.table(table(diagnosis, perimeter),2)
prop.table(table(diagnosis, area),2)
prop.table(table(diagnosis, smoothness),2)
prop.table(table(diagnosis, compactness),2)
prop.table(table(diagnosis, concavity),2)
prop.table(table(diagnosis, concave_points),2)
prop.table(table(diagnosis, symmetry),2)
prop.table(table(diagnosis, fractal_dimension),2)

#vizualize 
plot(diagnosis~texture,col=c("red","blue"))
plot(diagnosis~radius,col=c("red","blue"))
plot(diagnosis~perimeter,col=c("red","blue"))
plot(diagnosis~area,col=c("red","blue"))
plot(diagnosis~smoothness,col=c("red","blue"))
plot(diagnosis~compactness,col=c("red","blue"))
plot(diagnosis~concavity,col=c("red","blue"))
plot(diagnosis~concave_points,col=c("red","blue"))
plot(diagnosis~symmetry,col=c("red","blue"))
plot(diagnosis~fractal_dimension,col=c("red","blue"))

```

## PCA

```{r PCA}
pc.result<-prcomp(wdbc_sub[,3:12],scale.=TRUE)
pc.scores<-pc.result$x
pc.scores<-data.frame(pc.scores)
pc.scores$diagnosis<-wdbc_sub$diagnosis
view(pc.scores)

#Scree plot
eigenvals<-(pc.result$sdev)^2
plot(1:10,eigenvals/sum(eigenvals),type="l",main="Scree Plot PC's",ylab="Prop. Var. Explained",ylim=c(0,1))
cumulative.prop<-cumsum(eigenvals/sum(eigenvals))
lines(1:10,cumulative.prop,lty=2)


#Use ggplot2 to plot the first few pc's
ggplot(data = pc.scores, aes(x = PC1, y = PC2)) +
  geom_point(aes(col=diagnosis), size=1)+
  ggtitle("PCA of Cancer Status")

ggplot(data = pc.scores, aes(x = PC2, y = PC3)) +
  geom_point(aes(col=diagnosis), size=1)+
  ggtitle("PCA of Cancer Status")

ggplot(data = pc.scores, aes(x = PC1, y = PC3)) +
  geom_point(aes(col=diagnosis), size=1)+
  ggtitle("PCA of Cancer Status")
```


## Separation

PC1 vs PC2 and PC1 vs PC3 show good separation so some variable is a good predictor of cancer status.


## LDA & QDA

```{r LDA_QDA}

mylda<- lda(diagnosis ~ radius+texture+perimeter+area+smoothness+compactness+concavity+concave_points+symmetry+fractal_dimension, data = wdbc_sub)
myqda<- qda(diagnosis ~ radius+texture+perimeter+area+smoothness+compactness+concavity+concave_points+symmetry+fractal_dimension, data = wdbc_sub)

#confusion matrix
set.seed(12345)
index<-sample(1:569,455,replace=FALSE)
test.wdbc_sub<-wdbc_sub[-index,]
train.wdbc_sub<-wdbc_sub[index,]
prd<-predict(mylda, newdata = test.wdbc_sub)$class
table(prd,test.wdbc_sub$diagnosis)
mylda
#ROC
ldaprd<-predict(mylda, newdata = wdbc_sub)$posterior
#correcting for the way lda creates predicted probabilities
ldaprd<-ldaprd[,2]
summary (mylda)

pred <- prediction(ldaprd, wdbc_sub$diagnosis)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values

#Plot ROC
plot(roc.perf,main="LDA")
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))

#QDA
#confusion matrix
prd.q<-predict(myqda, newdata = test.wdbc_sub)$class
table(prd.q,test.wdbc_sub$diagnosis)
myqda
#ROC
qdaprd<-predict(myqda, newdata = wdbc_sub)$posterior
#correcting for the way lda creates predicted probabilities
qdaprd<-qdaprd[,2]


pred.q <- prediction(qdaprd, wdbc_sub$diagnosis)
roc.perf.q = performance(pred.q, measure = "tpr", x.measure = "fpr")
auc.train.q <- performance(pred.q, measure = "auc")
auc.train.q <- auc.train.q@y.values

#Plot ROC
plot(roc.perf,main="QDA")
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train.q[[1]],3), sep = ""))


# Export test and train data for SAS
setwd("~/Git/SMU/MSDS 6372/6372_proj2/Data")
write.csv(wdbc_sub, file = "wdbc_sub.csv")
write.csv(test.wdbc_sub, file = "test.wdbc_sub.csv")
write.csv(train.wdbc_sub, file = "train.wdbc_sub.csv")
```

ROC curve shows that the model does a good job reproducing results in training data
- no good interpretation
- ROC curve for our predictive model under the test set
- area under the curve is...

.0407 error rate
.0485 false negative
.0222 false positive

Sensitivity: 0.891089109 (correctly predict yes)
Specificity: 0.990825688 (correclty predict no)

## Stepwise Regression

```{r knn}
# test data created above test.wdbc_sub
# you don't get R^2 or other such parametric stats
ev.train.wdbc_sub <- train.wdbc_sub[,-c(1:2)]
diagnosis.train.wdbc_sub <- train.wdbc_sub[,c(2)]
ev.test.wdbc_sub <- test.wdbc_sub[,-c(1:2)]
diagnosis.test.wdbc_sub <- test.wdbc_sub[,c(2)]

# we'd rather tell people they have cancer than they dont. have more false positives
# bias vs variance tradeoff. p108

knn.pred.diagnosis <- knn(ev.train.wdbc_sub, ev.test.wdbc_sub, diagnosis.train.wdbc_sub, k=16)

table(knn.pred.diagnosis, diagnosis.test.wdbc_sub)
mean(knn.pred.diagnosis==diagnosis.test.wdbc_sub)

```

## Random

```{r random_forest}
short.tree<-tree(diagnosis~. ,train.wdbc_sub,mincut=50)
summary(short.tree)
plot(short.tree)
text(short.tree)


```



#TQ's Work Ends Here